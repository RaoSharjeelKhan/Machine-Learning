{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acd8aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rao\n",
      "[nltk_data]     Sharjeel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Rao\n",
      "[nltk_data]     Sharjeel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Rao\n",
      "[nltk_data]     Sharjeel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2223ffc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "df=df.iloc[:,0:2]\n",
    "df.columns=['target','text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72db0898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.countplot(data=df,\n",
    "              x=df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8a58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target=df.target.replace({'ham':0,'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55089d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fd9374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...   \n",
       "1       0                      Ok lar... Joking wif u oni...   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3       0  U dun say so early hor... U c already then say...   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...  \n",
       "1           [Ok, lar, ..., Joking, wif, u, oni, ...]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [U, dun, say, so, early, hor, ..., U, c, alrea...  \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text']=df['text'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c25bfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...   \n",
       "1       0                      Ok lar... Joking wif u oni...   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3       0  U dun say so early hor... U c already then say...   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                         clean_token  \n",
       "0  [Go, until, jurong, point, crazy, Available, o...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...  \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, he...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~...'\n",
    "def remove_punc (dataframe):\n",
    "    dataframe=list(c for c in dataframe if c not in punc)\n",
    "    return dataframe   \n",
    "df['clean_token']=df['tokenized_text'].apply(lambda x: remove_punc(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03e33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5156a460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_token_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, he...</td>\n",
       "      <td>[Nah, I, n't, think, goes, usf, lives, around,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...   \n",
       "1       0                      Ok lar... Joking wif u oni...   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3       0  U dun say so early hor... U c already then say...   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [Go, until, jurong, point, crazy, Available, o...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                      clean_token_sw  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3      [U, dun, say, early, hor, U, c, already, say]  \n",
       "4  [Nah, I, n't, think, goes, usf, lives, around,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def remove_sw (dataframe):\n",
    "    dataframe=list(c for c in dataframe if c not in stopwords)\n",
    "    return dataframe \n",
    "df['clean_token_sw']=df['clean_token'].apply(lambda x: remove_sw(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e4afcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_token_sw</th>\n",
       "      <th>clean_token_lm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, he...</td>\n",
       "      <td>[Nah, I, n't, think, goes, usf, lives, around,...</td>\n",
       "      <td>[Nah, I, n't, think, go, usf, life, around, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...   \n",
       "1       0                      Ok lar... Joking wif u oni...   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3       0  U dun say so early hor... U c already then say...   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [Go, until, jurong, point, crazy, Available, o...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                      clean_token_sw  \\\n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, n't, think, goes, usf, lives, around,...   \n",
       "\n",
       "                                      clean_token_lm  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3      [U, dun, say, early, hor, U, c, already, say]  \n",
       "4  [Nah, I, n't, think, go, usf, life, around, th...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn=nltk.WordNetLemmatizer()\n",
    "def lam (dataframe):\n",
    "    dataframe=list(wn.lemmatize(c) for c in dataframe)\n",
    "    return dataframe \n",
    "df['clean_token_lm']=df['clean_token_sw'].apply(lambda x: lam(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4bc7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>clean_token</th>\n",
       "      <th>clean_token_sw</th>\n",
       "      <th>clean_token_lm</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[Nah, I, do, n't, think, he, goes, to, usf, he...</td>\n",
       "      <td>[Nah, I, n't, think, goes, usf, lives, around,...</td>\n",
       "      <td>[Nah, I, n't, think, go, usf, life, around, th...</td>\n",
       "      <td>[nah, i, n't, think, go, usf, life, around, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...   \n",
       "1       0                      Ok lar... Joking wif u oni...   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3       0  U dun say so early hor... U c already then say...   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                         clean_token  \\\n",
       "0  [Go, until, jurong, point, crazy, Available, o...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, do, n't, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                      clean_token_sw  \\\n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, n't, think, goes, usf, lives, around,...   \n",
       "\n",
       "                                      clean_token_lm  \\\n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, n't, think, go, usf, life, around, th...   \n",
       "\n",
       "                                               lower  \n",
       "0  [go, jurong, point, crazy, available, bugis, n...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, early, hor, u, c, already, say]  \n",
       "4  [nah, i, n't, think, go, usf, life, around, th...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def low (dataframe):\n",
    "    dataframe=list(c.lower() for c in dataframe )\n",
    "    return dataframe \n",
    "df['lower']=df['clean_token_lm'].apply(lambda x: low(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33c9ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rao Sharjeel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>åè10</th>\n",
       "      <th>åð</th>\n",
       "      <th>åòharry</th>\n",
       "      <th>åòit</th>\n",
       "      <th>åômorrow</th>\n",
       "      <th>åôrents</th>\n",
       "      <th>ì_</th>\n",
       "      <th>ì¼1</th>\n",
       "      <th>ìä</th>\n",
       "      <th>ìï</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "0  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "1  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "2  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "3  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "4  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "\n",
       "   0125698789   02  ...  åè10   åð  åòharry  åòit  åômorrow  åôrents   ì_  \\\n",
       "0         0.0  0.0  ...   0.0  0.0      0.0   0.0       0.0      0.0  0.0   \n",
       "1         0.0  0.0  ...   0.0  0.0      0.0   0.0       0.0      0.0  0.0   \n",
       "2         0.0  0.0  ...   0.0  0.0      0.0   0.0       0.0      0.0  0.0   \n",
       "3         0.0  0.0  ...   0.0  0.0      0.0   0.0       0.0      0.0  0.0   \n",
       "4         0.0  0.0  ...   0.0  0.0      0.0   0.0       0.0      0.0  0.0   \n",
       "\n",
       "   ì¼1   ìä   ìï  \n",
       "0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 8289 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "tfidf2=TfidfVectorizer()\n",
    "x=tfidf2.fit_transform((df['lower']).astype(str))\n",
    "df_f=pd.DataFrame.sparse.from_spmatrix(x)\n",
    "df_f.columns=tfidf2.get_feature_names()\n",
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b91ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5d9a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0aea92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447236180904522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d7b477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1199,    3],\n",
       "       [  74,  117]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b1e3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "  text=text.lower()\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  text = remove_stopwords(text)\n",
    "  text=word_tokenize(text)\n",
    "  ps = PorterStemmer()\n",
    "  new=[]\n",
    "  for ele in range(len(text)):\n",
    "    new.append(ps.stem(text[ele]))\n",
    "  text=new\n",
    "  text=\" \".join(text)\n",
    "  return print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df93b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moddi buddi\n"
     ]
    }
   ],
   "source": [
    "text='How are moddi you buddy?'\n",
    "tnf=transform(text)\n",
    "tnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "687a8e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf2\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mtnf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(\u001b[38;5;28mstr\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "tfidf2.transform(tnf.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46bf89be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a8070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
